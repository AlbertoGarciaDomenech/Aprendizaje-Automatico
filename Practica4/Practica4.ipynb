{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 4 (Entrenamiento de redes neuronales)\n",
    "### Aprendizaje automático y big data\n",
    "##### _Alberto García Doménech - Pablo Daurell Marina_    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = loadmat('ex4weights.mat')\n",
    "\n",
    "theta1 = weights['Theta1'] # Shape: (25, 401)\n",
    "theta2 =weights['Theta2'] # Shape: (10, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos: 5000 ejemplos de entrenamiento (imagenes de 20x20 pixeles)\n",
    "data = loadmat('ex4data1.mat')\n",
    "\n",
    "X = data['X'] # Shape: (5000, 400)\n",
    "y = data['y'].ravel() # Shape: (5000,)\n",
    "\n",
    "num_entradas = np.shape(X)[1]\n",
    "num_etiquetas = 10 # Del 0 al 9\n",
    "num_ocultas = 25\n",
    "\n",
    "# Adaptamos 'y' para usarlo en la red neuronal\n",
    "y = (y - 1)\n",
    "y_onehot = np.zeros((len(y), num_etiquetas)) # Shape: (5000, 10)\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y_onehot[i][y[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    a1 = np.hstack([np.ones([m, 1]), X]) \n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.hstack([np.ones([m, 1]), sigmoid(z2)])\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    h = sigmoid(z3)\n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coste regularizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot(1 - y[i,:], np.log(1 - h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste sin regularización:  0.2876291651613187\n",
      "Coste con regularización:  0.3837698590909234\n"
     ]
    }
   ],
   "source": [
    "thetaVec = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "print('Coste sin regularización: ', backprop(thetaVec, num_entradas, num_ocultas, num_etiquetas, X, y_onehot, 0))\n",
    "print('Coste con regularización: ',backprop(thetaVec, num_entradas, num_ocultas, num_etiquetas, X, y_onehot, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesosAleatorios(L_in, L_out):\n",
    "    ini_epsilon = 0.12\n",
    "    theta = np.random.radom((L_out, 1 + L_in)) * (2*ini_epsilon) - ini_epsilon \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot(1 - y[i,:], np.log(1-h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "        \n",
    "    # Back-propagation\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]\n",
    "        a2t = a2[t,:]\n",
    "        ht = h[t,:]\n",
    "        yt = y[t]\n",
    "        \n",
    "        d3 = ht - yt\n",
    "        d2 = np.dot(theta2.T, d3) * (a2t * (1 - a2t))\n",
    "        \n",
    "        delta1 += np.dot(d2[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "        delta2 += np.dot(d3[:, np.newaxis], a2t[np.newaxis, :])\n",
    "        \n",
    "    # Calculo del gradiente\n",
    "    D1 = delta1 / m\n",
    "    D2 = delta2 / m\n",
    "    \n",
    "    gradient = np.concatenate((np.ravel(D1), np.ravel(D2)))\n",
    "    \n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27761168e-11,  1.89059467e-12,  7.89324509e-12,  6.95584909e-12,\n",
       "       -6.30465125e-11,  2.08456863e-12, -1.07556394e-11, -5.04682407e-11,\n",
       "       -9.07785513e-11,  7.04843475e-12, -3.98116679e-11, -1.22385352e-10,\n",
       "       -2.17855040e-11,  2.76547969e-12, -6.02735570e-12, -2.49761462e-11,\n",
       "        2.15736526e-11, -4.96176017e-13,  1.19978506e-11,  2.73879391e-11,\n",
       "        6.25964836e-11,  1.55131741e-11,  9.03210839e-12,  5.26763355e-12,\n",
       "        1.90088223e-11,  1.88441207e-11,  7.15513759e-11,  1.56080426e-11,\n",
       "        7.11190828e-12,  1.37491546e-11,  1.70987668e-11,  1.79336823e-11,\n",
       "        7.32915950e-11,  1.60134683e-11,  8.61832827e-12,  1.78091986e-11,\n",
       "        1.43913215e-11,  2.26750840e-11])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from checkNNGradients import checkNNGradients\n",
    "checkNNGradients(backprop, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente regularizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas, num_etiquetas, X, y, reg=0):\n",
    "    '''Funcion de back-propagation para red neuronal de 3 capas'''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Desplegamos los paramas_rn en la matrices Theta\n",
    "    theta1 = np.reshape(params_rn[:num_ocultas * (num_entradas + 1)],\n",
    "                       (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas * (num_entradas + 1):],\n",
    "                       (num_etiquetas, (num_ocultas + 1)))\n",
    "    \n",
    "    # Aplicamos forward-propagation para calcular la salidas de cada capa\n",
    "    a1, z2, a2, z3, h = forward_propagation(X, theta1, theta2)\n",
    "    \n",
    "    # Calculo del coste\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        a = np.dot(-y[i,:], np.log(h[i,:]))\n",
    "        b = np.dot((1-y[i,:]), np.log(1-h[i,:]))\n",
    "        cost += np.sum(a - b)\n",
    "        \n",
    "    cost = cost/m\n",
    "    \n",
    "    # Regularizacion del coste\n",
    "    cost += reg/(2*m) * (np.sum(theta1[:, 1:]**2) + np.sum(theta2[:, 1:]**2))\n",
    "    \n",
    "    # Back-propagation\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    \n",
    "    for t in range(m):\n",
    "        a1t = a1[t,:]\n",
    "        a2t = a2[t,:]\n",
    "        ht = h[t,:]\n",
    "        yt = y[t]\n",
    "        \n",
    "        d3 = ht - yt\n",
    "        d2 = np.dot(theta2.T, d3) * (a2t * (1 - a2t))\n",
    "        \n",
    "        delta1 += np.dot(d2[1:, np.newaxis], a1t[np.newaxis, :])\n",
    "        delta2 += np.dot(d3[:, np.newaxis], a2t[np.newaxis, :])\n",
    "        \n",
    "    # Calculo del gradiente\n",
    "    D1 = delta1 / m\n",
    "    D2 = delta2 / m\n",
    "    \n",
    "    # Regularizacion del gradiente\n",
    "    D1[:, 1:] = D1[:, 1:] + (reg * theta1[:, 1:]) / m\n",
    "    D2[:, 1:] = D2[:, 1:] + (reg * theta2[:, 1:]) / m\n",
    "    \n",
    "    gradient = np.concatenate((np.ravel(D1), np.ravel(D2)))\n",
    "    \n",
    "    return cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.27761168e-11,  7.32719441e-13,  8.82988127e-12,  7.53047624e-12,\n",
       "       -6.30465125e-11,  2.10970130e-12, -1.16537613e-11, -4.92537400e-11,\n",
       "       -9.07785513e-11,  5.59484403e-12, -3.90588950e-11, -1.22203025e-10,\n",
       "       -2.17855040e-11,  4.35645964e-12, -7.00919878e-12, -2.43030734e-11,\n",
       "        2.15736526e-11,  2.27623476e-13,  1.19978506e-11,  2.84505197e-11,\n",
       "        6.25964836e-11,  1.38673517e-11,  8.50600146e-12,  5.29278010e-12,\n",
       "        2.03311395e-11,  1.78381754e-11,  7.15513759e-11,  1.63749014e-11,\n",
       "        7.86468113e-12,  1.39315087e-11,  1.64833286e-11,  1.95246597e-11,\n",
       "        7.32915950e-11,  1.66865410e-11,  8.55090998e-12,  1.63125624e-11,\n",
       "        1.34624811e-11,  2.22044327e-11])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from checkNNGradients import checkNNGradients\n",
    "checkNNGradients(backprop, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
